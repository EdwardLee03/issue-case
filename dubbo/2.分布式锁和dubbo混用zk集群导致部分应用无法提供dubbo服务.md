

分布式锁和dubbo混用zk集群导致部分应用无法提供dubbo服务
=================================
> 盘竹、错刀，2017-05-25


## 1.现象
- 业务方反馈5.8早上凌晨1点多，部分应用无法提供dubbo服务

日志
```
2017-05-08 01:16:25,336 WARN [kafka-consumer-heartbeat-loanbackend.alps.syn] -------------.
2017-05-08 01:16:30,288 WARN [New I/O worker #14] c.a.d.r.transport.AbstractServer - [] [] [DUBBO] All clients has discontected from /*:*. You can graceful shutdown now., dubbo version: 2.5.3, current host: *
```


## 2.原因分析
- `业务方使用dubbo的zk集群做分布式锁功能，用户中心那边请求量过多，导致了FGC和zk选举发生`。
- 2017-05-08 01:04:30,207 zk集群发生选举。
- 应用系统A获取不到zk数据，并尝试重连
- 2017-05-08 01:06:09 应用接收到zookeeper1的session1过期请求；
- dubbo开启重试机制，重新注册服务信息；
- 应用开始重新同zk建立连接；
- 2017-05-08 01:06:11,652 zookeeper3选举成功；
- 2017-05-08 01:10:21,122 应用同zookeeper3建立起连接(session2)；
- zookeeper3开始处理dubbo服务注册请求；
- 由于session1在zookeeper3中过期流程正在处理过程中；session1的临时节点并没有删除，导致dubbo重新注册都是失败的(KeeperErrorCode = NodeExists)；
- dubbo注册流程完成；
- session1在zookeeper3过期流程处理结束，删除临时节点；
- 其他业务系统监听到了应用系统A节点变更通知，移除该应用系统对应服务，最终导致应用系统A不再处理dubbo请求。


## 3.解决&改进
- dubbo在监听到zk session过期请求，进行重连时，首先检查一下zk上面对应的服务是否存在，如果存在(由于session过期，此节点应该是无效的)，则删除，并重新进行注册。(待开发)
- 搭建独立的一套ZK集群，提供给分布式锁等非dubbo相关的场景使用。


> 错刀

排查问题的过程中，发现`很多应用使用了dubbo注册中心所依赖的zk集群，作分布式锁功能`；
curator框架在创建分布式锁的时候，并不会删除锁节点，这样就导致了zk中产生了很多的废弃的临时节点。
线上dubbo的zk集群，只有一个log文件，汇总了从今年2月到现在的日志信息(大小目前7G以上)，导致日志查询困难。

